{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0a98fc",
   "metadata": {},
   "source": [
    "# Agriculture Detection with Keras\n",
    "This notebook focuses on the TensorFlow/Keras workflow for loading satellite imagery data, batching it, and preparing it for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import importlib\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Local imports\n",
    "import utils.main_utils as main_utils\n",
    "importlib.reload(main_utils)\n",
    "from utils.main_utils import check_skillnetwork_extraction, shuffle_data\n",
    "import utils.keras_ai_utils as keras_ai_utils\n",
    "importlib.reload(keras_ai_utils)\n",
    "from utils.keras_ai_utils import (\n",
    "    set_tf_processing_env,\n",
    "    create_keras_datasets,\n",
    "    display_keras_batch,\n",
    "    configure_keras_for_performance,\n",
    "    keras_custom_data_generator,\n",
    "    display_custom_keras_batch,\n",
    "    create_generators,\n",
    "    build_keras_model,\n",
    "    display_keras_history,\n",
    "    evaluate_keras_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce412818",
   "metadata": {},
   "source": [
    "<!-- ## Download and Extract Data\n",
    "Download the satellite images dataset and extract it to the `data/` directory if it is not already available. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560068e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract data\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar'\n",
    "extract_dir = './data/tf_data/'\n",
    "model_dir = './models/'\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "dataset_path = os.path.join(extract_dir, 'images_dataSAT')\n",
    "await check_skillnetwork_extraction(extract_dir, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125495a",
   "metadata": {},
   "source": [
    "## Label and Shuffle Data\n",
    "Aggregate image paths and labels for agricultural and non-agricultural classes, then shuffle for downstream batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label and shuffle data\n",
    "base_dir = './data/tf_data/images_dataSAT/'\n",
    "dir_non_agri_name = os.path.join(base_dir, 'class_0_non_agri')\n",
    "dir_agri_name = os.path.join(base_dir, 'class_1_agri')\n",
    "\n",
    "all_image_paths, all_labels = shuffle_data(dir_non_agri_name, dir_agri_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e1723",
   "metadata": {},
   "source": [
    "## Custom Data Generator vs Keras Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generator for custom batching\n",
    "batch_size = 8\n",
    "data_generator = keras_custom_data_generator(\n",
    "    image_paths=all_image_paths,\n",
    "    labels=all_labels,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "display_custom_keras_batch(data_generator, batch_size, title='Custom Generator')\n",
    "\n",
    "# Create and inspect Keras datasets\n",
    "img_size = (64, 64)\n",
    "train_ds, val_ds = create_keras_datasets(\n",
    "    base_dir=base_dir,\n",
    "    img_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "display_keras_batch(batch_size, train_ds, title='Keras Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47501fb",
   "metadata": {},
   "source": [
    "## Configure Keras Pipelines for Performance\n",
    "Apply caching, shuffling, and prefetching strategies to optimize input pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize datasets for performant training\n",
    "train_ds, val_ds = configure_keras_for_performance(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af0def",
   "metadata": {},
   "source": [
    "## Setup Tensorflow Processing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, fnames = set_tf_processing_env(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63e6a0",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3babc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Parameters\n",
    "img_w, img_h = 64, 64                  # Image width and height\n",
    "n_channels = 3                         # Number of image channels (e.g., 3 for RGB)\n",
    "hidden_activation = 'relu'             # Activation function for hidden layers\n",
    "output_activation = 'sigmoid'          # Activation function for the output layer\n",
    "padding = 'same'                       # Padding strategy for convolutional layers\n",
    "strides = (1, 1)                       # Strides for convolutional layers\n",
    "kernel_size = (3, 3)                   # Kernel size for convolutional layers\n",
    "kernel_initializer = HeUniform()       # Initializer for the kernel weights\n",
    "kernel_regularizer = l2(0.001)         # Regularizer for the kernel weights\n",
    "dropout = .2                           # Dropout rate for regularization\n",
    "filter_base = 32                       # Base number of filters for convolutional layers\n",
    "unit_base = 128                        # Base number of units for dense layers\n",
    "output_units = 1                       # Number of units in the output layer\n",
    "conv_block_num = 3                     # Number of convolutional blocks\n",
    "dense_block_num = 1                    # Number of dense blocks\n",
    "pool_size = (2, 2)                     # Pool size for MaxPooling layers\n",
    "pool_strides = (2, 2)                  # Strides for MaxPooling layers\n",
    "\n",
    "# Training Hyperparameters\n",
    "lr = .001                              # Learning rate for the optimizer\n",
    "n_epochs = 10                          # Number of times to iterate over the entire dataset\n",
    "batch_size = 128                       # Number of samples per gradient update\n",
    "steps_per_epoch = None                 # Number of steps (batches) to take for each epoch\n",
    "validation_steps = None                # Number of steps (batches) to take for validation\n",
    "loss = 'binary_crossentropy'           # Loss function for the model\n",
    "optimizer = Adam(learning_rate=lr)     # Optimizer for training the model\n",
    "metrics = ['accuracy']                 # Metrics to monitor during training\n",
    "\n",
    "# Data Augmentation Parameters\n",
    "rescale = 1./255                       # Rescaling factor for pixel values\n",
    "rotation_range = 20                    # Degree range for random rotations\n",
    "width_shift_range = 0.1                # Fraction of total width for random horizontal shifts\n",
    "height_shift_range = 0.1               # Fraction of total height for random vertical shifts\n",
    "shear_range = 0.1                      # Shear intensity (shear angle in counter-clockwise direction)\n",
    "zoom_range = 0.075                     # Range for random zoom\n",
    "horizontal_flip = True                 # Randomly flip inputs horizontally\n",
    "fill_mode = 'nearest'                  # Strategy for filling in newly created pixels\n",
    "validation_split = 0.2                 # Fraction of data to reserve for validation\n",
    "\n",
    "# Configuration Parameters\n",
    "model_name = os.path.join(             # Name of the file to save the model\n",
    "    model_dir, \n",
    "    'keras_model.model.keras'\n",
    ") \n",
    "class_mode = 'binary'                  # Type of classification problem\n",
    "target_size = (img_w, img_h)           # Dimensions to which all images will be resized\n",
    "tqdm_verbose = 1                       # Verbosity mode for TQDM progress bar\n",
    "verbose = 0                            # Verbosity mode for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44504d35",
   "metadata": {},
   "source": [
    "## Create Data Generators for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefaacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, validation_generator = create_generators(\n",
    "    dataset_path,\n",
    "    target_size,\n",
    "    batch_size,\n",
    "    class_mode,\n",
    "    rescale,\n",
    "    rotation_range,\n",
    "    width_shift_range,\n",
    "    height_shift_range,\n",
    "    shear_range,\n",
    "    zoom_range,\n",
    "    horizontal_flip,\n",
    "    fill_mode,\n",
    "    validation_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f9fc1",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, fit = build_keras_model(\n",
    "    train_generator,\n",
    "    validation_generator,\n",
    "    optimizer,\n",
    "    kernel_initializer, \n",
    "    filter_base=filter_base, \n",
    "    unit_base=unit_base, \n",
    "    output_units=output_units, \n",
    "    conv_block_num=conv_block_num,\n",
    "    dense_block_num=dense_block_num,\n",
    "    hidden_activation=hidden_activation, \n",
    "    padding=padding, \n",
    "    strides=strides, \n",
    "    kernel_size=kernel_size, \n",
    "    img_w=img_w, \n",
    "    img_h=img_h, \n",
    "    n_channels=n_channels, \n",
    "    dropout=dropout, \n",
    "    output_activation=output_activation,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    n_epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    tqdm_verbose=tqdm_verbose,\n",
    "    verbose=verbose,\n",
    "    validation_steps=validation_steps,\n",
    "    pool_size=pool_size,\n",
    "    pool_strides=pool_strides\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cbda9",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348324da",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_keras_history(validation_generator, model, fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc4e48",
   "metadata": {},
   "source": [
    "## Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_keras_model(model, validation_generator, device, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
